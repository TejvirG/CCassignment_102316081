{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c328289a",
   "metadata": {},
   "source": [
    "# NLP Assignment\n",
    "Using a sample paragraph on **Technology**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c706469",
   "metadata": {},
   "source": [
    "## Q1. Text Processing and Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94849b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"\"\"Technology is evolving faster than ever. Innovations like artificial intelligence, quantum computing, \n",
    "and blockchain are transforming industries. I find it fascinating how algorithms impact daily life. \n",
    "From smartphones to smart homes, tech makes everything more connected. The future promises even more \n",
    "integration of digital systems into society.\"\"\"\n",
    "\n",
    "lower_text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "words = word_tokenize(lower_text)\n",
    "sentences = sent_tokenize(lower_text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "print(\"Tokenized Sentences:\", sentences)\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "print(\"Word Frequencies:\", word_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5298b4",
   "metadata": {},
   "source": [
    "## Q2. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "porter_result = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_result = [lancaster.stem(word) for word in filtered_words]\n",
    "lemma_result = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Porter Stemming:\", porter_result)\n",
    "print(\"Lancaster Stemming:\", lancaster_result)\n",
    "print(\"Lemmatization:\", lemma_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c9b10",
   "metadata": {},
   "source": [
    "## Q3. Regular Expressions and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "words_5plus = re.findall(r'\\b\\w{6,}\\b', text)\n",
    "numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "\n",
    "only_alpha = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "vowel_words = [word for word in only_alpha if word[0].lower() in 'aeiou']\n",
    "\n",
    "print(\"Words with >5 letters:\", words_5plus)\n",
    "print(\"Numbers:\", numbers)\n",
    "print(\"Capitalized Words:\", capitalized)\n",
    "print(\"Only Alphabet Words:\", only_alpha)\n",
    "print(\"Words Starting with Vowel:\", vowel_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520e44a",
   "metadata": {},
   "source": [
    "## Q4. Custom Tokenization & Regex Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a60a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9.\\s\\-\\'â€™]', ' ', text)\n",
    "    tokens = re.findall(r\"\\d+\\.\\d+|\\b\\w[\\w\\-']*\\b\", text)\n",
    "    return tokens\n",
    "\n",
    "sample_text = \"\"\"Email me at hello@example.com or visit https://technews.com. \n",
    "Call +91 9876543210 for more info. Our system is state-of-the-art, and isn't it amazing?\"\"\"\n",
    "\n",
    "cleaned_text = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w+\\b', '<EMAIL>', sample_text)\n",
    "cleaned_text = re.sub(r'https?://[^\\s]+', '<URL>', cleaned_text)\n",
    "cleaned_text = re.sub(r'(\\+?\\d{1,3})?[-.\\s]?\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '<PHONE>', cleaned_text)\n",
    "\n",
    "tokens = custom_tokenizer(cleaned_text)\n",
    "\n",
    "print(\"Cleaned Text:\", cleaned_text)\n",
    "print(\"Custom Tokens:\", tokens)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}