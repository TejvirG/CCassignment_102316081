{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1947634",
   "metadata": {},
   "source": [
    "# NLP Assignment - Advanced Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93237e53",
   "metadata": {},
   "source": [
    "## Q1. Text Cleaning and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"\"\"Food has always fascinated me, not just as a necessity but as an art form. The variety of flavors, textures, and ingredients used across cultures is amazing. I love exploring new cuisines, especially street food which tells a story of its region. Cooking at home has become a creative outlet and a form of relaxation. Food also brings people together, whether itâ€™s a family dinner or a festive feast. I believe food is not just fuel but a universal language.\"\"\"\n",
    "\n",
    "cleaned = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "words_split = cleaned.split()\n",
    "words_token = word_tokenize(cleaned)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [w for w in words_token if w not in stop_words]\n",
    "\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "print(\"Original Sentences:\", sentences)\n",
    "print(\"Split() Words:\", words_split)\n",
    "print(\"word_tokenize() Words:\", words_token)\n",
    "print(\"Stopword Removed:\", filtered_words)\n",
    "print(\"Word Frequencies:\", word_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0006dcc",
   "metadata": {},
   "source": [
    "## Q2. Regex, Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1758ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "words_alpha = re.findall(r'\\b[a-zA-Z]+\\b', cleaned)\n",
    "filtered_alpha = [w for w in words_alpha if w not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stems = [stemmer.stem(w) for w in filtered_alpha]\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in filtered_alpha]\n",
    "\n",
    "print(\"Alphabet Words:\", words_alpha)\n",
    "print(\"Filtered Words:\", filtered_alpha)\n",
    "print(\"Stemming:\", stems)\n",
    "print(\"Lemmatization:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767145ef",
   "metadata": {},
   "source": [
    "## Q3. Bag of Words and TF-IDF with Short Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de861165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"Apple unveils new iPhone with advanced AI features.\",\n",
    "    \"Users praise battery life in the latest Android phones.\",\n",
    "    \"Camera quality in smartphones continues to improve yearly.\"\n",
    "]\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "count_matrix = count_vec.fit_transform(texts)\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vec.fit_transform(texts)\n",
    "\n",
    "feature_names = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Top 3 TF-IDF keywords for Text {i+1}:\")\n",
    "    row = tfidf_matrix[i].toarray()[0]\n",
    "    top_indices = row.argsort()[-3:][::-1]\n",
    "    for idx in top_indices:\n",
    "        print(f\"{feature_names[idx]}: {row[idx]:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a20b59",
   "metadata": {},
   "source": [
    "## Q4. Similarity Between Technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895af66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tech1 = \"\"\"Artificial Intelligence is revolutionizing industries by automating tasks and making decisions.\"\"\"\n",
    "tech2 = \"\"\"Blockchain ensures secure, decentralized data storage and is changing finance and supply chains.\"\"\"\n",
    "\n",
    "tokens1 = set(re.findall(r'\\b\\w+\\b', tech1.lower()))\n",
    "tokens2 = set(re.findall(r'\\b\\w+\\b', tech2.lower()))\n",
    "\n",
    "jaccard = len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "print(\"Jaccard Similarity:\", round(jaccard, 3))\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform([tech1, tech2])\n",
    "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "print(\"Cosine Similarity:\", round(cos_sim[0][0], 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9bdf8",
   "metadata": {},
   "source": [
    "## Q5. Sentiment Analysis and WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "review = \"The customer service was excellent and the product quality exceeded my expectations.\"\n",
    "\n",
    "blob = TextBlob(review)\n",
    "polarity = blob.sentiment.polarity\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "sentiment = \"Positive\" if polarity > 0 else \"Negative\" if polarity < 0 else \"Neutral\"\n",
    "\n",
    "print(\"Polarity:\", polarity)\n",
    "print(\"Subjectivity:\", subjectivity)\n",
    "print(\"Sentiment:\", sentiment)\n",
    "\n",
    "if sentiment == \"Positive\":\n",
    "    wc = WordCloud(width=400, height=200).generate(review)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204acc2",
   "metadata": {},
   "source": [
    "## Q6. Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "train_text = \"\"\"Technology continues to evolve rapidly, introducing new tools and platforms that reshape the world. \n",
    "Every innovation brings unique opportunities and challenges. Developers and engineers are at the forefront of change.\"\"\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([train_text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in train_text.split('.'):\n",
    "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(tokens)):\n",
    "        seq = tokens[:i+1]\n",
    "        input_sequences.append(seq)\n",
    "\n",
    "max_seq_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len)\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "seed_text = \"Technology continues\"\n",
    "for _ in range(3):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1)\n",
    "    predicted = model.predict(token_list, verbose=0).argmax(axis=1)[0]\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            seed_text += ' ' + word\n",
    "            break\n",
    "\n",
    "print(\"Generated Text:\", seed_text)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}